{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/regression-part2-automated-ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Catboost Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you use  machine learning in Azure Machine Learning to create a regression model leveraging the CatBoot python library\n",
    "\n",
    "* Download, transform, and clean data using Azure Open Datasets\n",
    "* Train an machine learning linear regression model\n",
    "* Deploy your model with ACI & AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azureml.core import Dataset\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv')\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Item_Weight'].fillna(value=df_train['Item_Weight'].mean(), inplace=True)\n",
    "df_train['Outlet_Size'].fillna(value='unavailable', inplace=True)\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os. getcwd()\n",
    "print(cwd)\n",
    "dataset_name = user + '-bigmart-train.csv'\n",
    "print(dataset_name)\n",
    "dataset_dir = './register/'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "file_path = os.path.join(dataset_dir, dataset_name)\n",
    "df_train.to_csv(file_path, index=False)\n",
    "\n",
    "#upload to datastore\n",
    "from azureml.core.datastore import Datastore\n",
    "ds = Datastore.get_default(ws)\n",
    "ds.upload('register/', target_path='data/prepped', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "#create a dataset object from the uploaded file\n",
    "dataset_name = user + '-bigmart-train'\n",
    "dataset_file =  dataset_name + '.csv'\n",
    "dataset = Dataset.Tabular.from_delimited_files(ds.path('data/prepped/' + dataset_file))\n",
    "#register dataset\n",
    "dataset.register(ws, dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_name)\n",
    "train_dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "label = \"Item_Outlet_Sales\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure workspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a workspace object from the existing workspace. A [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py) is a class that accepts your Azure subscription and resource information. It also creates a cloud resource to monitor and track your model runs. `Workspace.from_config()` reads the file **config.json** and loads the authentication details into an object named `ws`. `ws` is used throughout the rest of the code in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will save into a register folder the data set that we are going to register for later use. Notice that we have now created a new folder that holds the dataset we would like to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an experiment object in your workspace. An experiment acts as a container for your individual runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, user + \"-catboost-exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"train\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Training file\n",
    "\n",
    "Below be use to update the train.py file to **write your user name**\n",
    "\n",
    "This train script will create a trained model that has been saved to your run outputs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.run import Run\n",
    "from azureml.core import Dataset\n",
    "from azureml.core import Workspace\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def getRuntimeArgs():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_user', type=str)\n",
    "    args = parser.parse_args()\n",
    "    print(\"Argument: %s\" % args.data_user)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    print('In main')\n",
    "    \n",
    "    print('About to get args')\n",
    "    args = getRuntimeArgs()\n",
    "    \n",
    "    print(\"Argument 1: %s\" % args.data_user)\n",
    "    \n",
    "    run = Run.get_context()\n",
    "\n",
    "    print(\"got run context\")\n",
    "    \n",
    "    dataset_dir = './dataset/'\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    ws = run.experiment.workspace\n",
    "    print(ws)\n",
    "\n",
    "    dataset_name = args.data_user + '-bigmart-train'\n",
    "    \n",
    "    print('dataset name:' + dataset_name)\n",
    "    \n",
    "    dataset_lt = Dataset.get_by_name(ws, name=dataset_name)\n",
    "    \n",
    "    # Load a TabularDataset & save into pandas DataFrame\n",
    "    df = dataset_lt.to_pandas_dataframe()\n",
    "    df.to_csv(os.path.join(dataset_dir, 'dataset.csv'), index = False)\n",
    "    \n",
    "\n",
    "    mod = model_train(df, run)\n",
    "    #copying to \"outputs\" directory, automatically uploads it to Azure ML\n",
    "    output_dir = './outputs/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_name = os.path.join(output_dir, 'cat-model')\n",
    "    mod.save_model(model_name)\n",
    "\n",
    "def model_train(ds_df, run):\n",
    "\n",
    "    y_raw = ds_df['Item_Outlet_Sales']\n",
    "    X_raw = ds_df.drop('Item_Outlet_Sales', axis=1)\n",
    "\n",
    "    # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    categorical_features_indices = np.where(X_raw.dtypes != np.float)[0]\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=50, depth=2, learning_rate=0.1, loss_function='RMSE')\n",
    "    model.fit(X_train, y_train, cat_features=categorical_features_indices, eval_set=(X_test, y_test), plot=False)\n",
    "    \n",
    "    # Capture metrics\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(\"Training accuracy: %.3f\" % train_acc)\n",
    "    print(\"Test data accuracy: %.3f\" % test_acc)\n",
    "\n",
    "    # Log to Azure ML\n",
    "    run.log('Train accuracy', train_acc)\n",
    "    run.log('Test accuracy', test_acc)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "print(user)\n",
    "compute_name = user + \"-cluster\"\n",
    "print(compute_name)\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D13\",\n",
    "                                                   min_nodes=0, \n",
    "                                                   max_nodes=1)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Run Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "dependencies = CondaDependencies()\n",
    "dependencies.add_pip_package('numpy==1.17.0')\n",
    "dependencies.add_pip_package('joblib==0.14.1')\n",
    "dependencies.add_pip_package('scikit-learn')\n",
    "dependencies.add_pip_package('Catboost')\n",
    "\n",
    "#Create a Run Configuration and add this to your pythonscriptstep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "run_config = RunConfiguration()\n",
    "run_config.target = compute_name\n",
    "run_config.environment.python.conda_dependencies = dependencies\n",
    "run_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select your training script and create a ScriptRunConfig\n",
    "A ScriptRunConfig object packages together the environment from a RunConfiguration along with your model training script. This object can then be submitted to your experiment and model training will commence on your remote cluster. \n",
    "\n",
    "In this sample, we have put the training script in a separate directory which is targeted for training. This separation allows for a snapshot of just the relevant pieces of code to be stored with the Run in your AML workspace. The <code>train.py</code> file here accesses your registered datasets, trains a model, saves a pickled version, and registers the trained model.\n",
    "\n",
    "ScriptRunConfiguration documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory='./train',\n",
    "                            script='train.py',\n",
    "                            arguments=['--data_user', user])\n",
    "src.run_config = run_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training run\n",
    "Here, the ScriptRunConfiguration is submitted as a run which triggers your model training operation. The cluster you defined above is automatically spun up and the training procedures outlined in ./train/train.py begin. That file contains all the code needed to train and save a pickled version of your trained model. The code below will display the output logs from your training job - you can also monitor training progress inside AML studio.\n",
    "\n",
    "Note: As you iterate on your model, you should modify the code inside ./train/train.py. The model parameters there were adjusted for rapid training and should not be used for a production scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "run = experiment.submit(config=src)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"score\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    # Update to your model's filename\n",
    "    model_filename = \"cat-model\"\n",
    "\n",
    "    # AZUREML_MODEL_DIR is injected by AML\n",
    "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
    "\n",
    "    print(\"Model dir:\", model_dir)\n",
    "    print(\"Model filename:\", model_filename)\n",
    "    \n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "\n",
    "    \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(model_path)\n",
    "\n",
    "input_sample = [{\n",
    "        \"Item_Identifier\" : \"FDA15\",\n",
    "        \"Item_Weight\":9.3,\n",
    "        \"Item_Fat_Content\": \"Low Fat\",\n",
    "        \"Item_Visibility\": 0.016047,\n",
    "        \"Item_Type\" : \"Dairy\",\n",
    "        \"Item_MRP\" : 249.8092,\n",
    "        \"Outlet_Identifier\": \"OUT049\",\n",
    "        \"Outlet_Establishment_Year\": 1999, \n",
    "        \"Outlet_Size\": \"Medium\", \n",
    "        \"Outlet_Location_Type\": \"Tier 1\", \n",
    "        \"Outlet_Type\": \"Supermarket Type1\",\n",
    "        \"Item_Outlet_Sales\": 3735.1380\n",
    "        }\n",
    "]\n",
    "output_sample = [8880.0]\n",
    "\n",
    "# This will automatically unmarshall the data parameter in the HTTP request\n",
    "@input_schema('data', StandardPythonParameterType(input_sample))\n",
    "@output_schema(StandardPythonParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        input_df = pd.DataFrame(data)\n",
    "        proba = model.predict(input_df)\n",
    "        \n",
    "        result = {\"result\": proba.tolist()}\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model_name = user + '-cat-boost'\n",
    "trained_model = run.register_model(model_path='outputs/cat-model', model_name=model_name, tags={'Model Type': 'cat-boost regression'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "env = Environment('tutorial-env')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataprep[pandas,fuse]>=1.1.14', 'azureml-defaults', 'catboost', 'inference-schema'], conda_packages = ['scikit-learn==0.22.1'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "\n",
    " You can register this model and deploy it to an endpoint by defining an inferencing configuration and providing a scoring script. Here the model is deployed to an Azure Container Instance which provides an API endpoint that can be used to make predictions with your model. We utilize an authentication strategy here which requires a key to be provided with any requests sent to the API. These keys can be rotated as needed and allow only approved users to access your endpoint.\n",
    " \n",
    " Azure Container Instance documentation: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance\n",
    "\n",
    "Azure Container Instances are typically lower cost and useful for dev/test purposes during model development, though we recommend deploying to an Azure Kubernetes Service cluster for production purposes.\n",
    "\n",
    "Below, an InferenceConfig is created which uses the same python dependencies that were used during model training, and references the scoring script located at <code>.score.py</code>. This script loads the trained model upon initialization, and facilitates transforming data submitted to the API endpoint, making predictions with the model, and returning formatted results to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               auth_enabled=False,\n",
    "                                               tags={\"data\": \"mart\",  \"method\" : \"cat-boost\"}, \n",
    "                                               description='cat-boost-demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = Environment.list(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workspaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your model and deploy to an authenticated endpoint \n",
    "\n",
    "Model registration documentation: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('model_name:'+ model_name)\n",
    "\n",
    "model = Model(ws, model_name)\n",
    "\n",
    "\n",
    "myenv = Environment.get(workspace=ws, name=\"tutorial-env\", version=None)\n",
    "inference_config = InferenceConfig(source_directory='.', entry_script=\"score.py\", environment=myenv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your model toACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(workspace=ws, \n",
    "                       name=model_name +'-service4', \n",
    "                       models=[model], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scoring API available at: {}'.format(service.serialize()['scoringUri']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#label = \"Item_Outlet_Sales\"\n",
    "\n",
    "input_payload = json.dumps({\n",
    "  \"data\": [{\n",
    "        \"Item_Identifier\" : \"FDA15\",\n",
    "        \"Item_Weight\":9.3,\n",
    "        \"Item_Fat_Content\": \"Low Fat\",\n",
    "        \"Item_Visibility\": 0.016047,\n",
    "        \"Item_Type\" : \"Dairy\",\n",
    "        \"Item_MRP\" : 249.8092,\n",
    "        \"Outlet_Identifier\": \"OUT049\",\n",
    "        \"Outlet_Establishment_Year\": 1999, \n",
    "        \"Outlet_Size\": \"Medium\", \n",
    "        \"Outlet_Location_Type\": \"Tier 1\", \n",
    "        \"Outlet_Type\": \"Supermarket Type1\",\n",
    "        }\n",
    "]\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "endpoint_url = service.serialize()['scoringUri']\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "body =  {\n",
    "  \"data\": [{\n",
    "        \"Item_Identifier\" : \"FDA15\",\n",
    "        \"Item_Weight\":9.3,\n",
    "        \"Item_Fat_Content\": \"Low Fat\",\n",
    "        \"Item_Visibility\": 0.016047,\n",
    "        \"Item_Type\" : \"Dairy\",\n",
    "        \"Item_MRP\" : 249.8092,\n",
    "        \"Outlet_Identifier\": \"OUT049\",\n",
    "        \"Outlet_Establishment_Year\": 1999, \n",
    "        \"Outlet_Size\": \"Medium\", \n",
    "        \"Outlet_Location_Type\": \"Tier 1\", \n",
    "        \"Outlet_Type\": \"Supermarket Type1\",\n",
    "        }\n",
    "]\n",
    "}\n",
    "r = requests.post(endpoint_url, headers=headers, data=json.dumps(body))\n",
    "results = r.json()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve access keys for your API endpoint\n",
    "An authentication mechanism was added to the API endpoint that requires a unique key be provided with any requests to the API. These keys can be programmatically retrieved by users who have access to the AML workspace, or retrieved manually from the workspace. It is worth noting, these keys can be rotated at your discretion and old keys will no longer work.\n",
    "\n",
    "Webservice documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy AKS Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-mm'\n",
    "\n",
    "# Uses the specific FPGA enabled VM (sku: Standard_PB6s)\n",
    "# Standard_PB6s are available in: eastus, westus2, westeurope, southeastasia\n",
    "prov_config = AksCompute.provisioning_configuration(vm_size = \"Standard_D3_v2\",\n",
    "                                                       agent_count = 1,\n",
    "                                                       location = \"eastus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "service = Model.deploy(ws, \"cat-boost-sv\", [model], inference_config, deployment_config, aks_target, overwrite=True)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1, Key2 = service.get_keys()\n",
    "print(key1)\n",
    "selected_key = key1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "endpoint_url = service.serialize()['scoringUri']\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer {}\".format(selected_key),\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "body =  {\n",
    "  \"data\": [{\n",
    "        \"Item_Identifier\" : \"FDA15\",\n",
    "        \"Item_Weight\":9.3,\n",
    "        \"Item_Fat_Content\": \"Low Fat\",\n",
    "        \"Item_Visibility\": 0.016047,\n",
    "        \"Item_Type\" : \"Dairy\",\n",
    "        \"Item_MRP\" : 249.8092,\n",
    "        \"Outlet_Identifier\": \"OUT049\",\n",
    "        \"Outlet_Establishment_Year\": 1999, \n",
    "        \"Outlet_Size\": \"Medium\", \n",
    "        \"Outlet_Location_Type\": \"Tier 1\", \n",
    "        \"Outlet_Type\": \"Supermarket Type1\",\n",
    "        },\n",
    "       {\n",
    "        \"Item_Identifier\" : \"FDA15\",\n",
    "        \"Item_Weight\":9.3,\n",
    "        \"Item_Fat_Content\": \"Low Fat\",\n",
    "        \"Item_Visibility\": 0.016047,\n",
    "        \"Item_Type\" : \"Dairy\",\n",
    "        \"Item_MRP\" : 249.8092,\n",
    "        \"Outlet_Identifier\": \"OUT049\",\n",
    "        \"Outlet_Establishment_Year\": 1999, \n",
    "        \"Outlet_Size\": \"Medium\", \n",
    "        \"Outlet_Location_Type\": \"Tier 1\", \n",
    "        \"Outlet_Type\": \"Supermarket Type1\",\n",
    "        }\n",
    "]\n",
    "}\n",
    "r = requests.post(endpoint_url, headers=headers, data=json.dumps(body))\n",
    "results = r.json()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "jeffshep"
   }
  ],
  "categories": [
   "tutorials"
  ],
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "msauthor": "trbye",
  "network_required": false,
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
